{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rU4x4KbAboO",
        "outputId": "d9d54ef7-9f04-4f93-c819-da8ee3f46356"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/180.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”\u001b[0m \u001b[32m174.1/180.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m180.0/180.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m485.1/485.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m137.3/137.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m473.8/473.8 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.1.0 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-text-splitters<1.0.0,>=0.3.9, but you have langchain-text-splitters 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.1.0 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-text-splitters<1.0.0,>=0.3.9, but you have langchain-text-splitters 1.0.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m120.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:5 https://cli.github.com/packages stable InRelease\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,532 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,151 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,222 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,876 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,475 kB]\n",
            "Get:17 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,832 kB]\n",
            "Fetched 28.5 MB in 3s (9,260 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 42 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q yt-dlp\n",
        "!pip install -q youtube-transcript-api\n",
        "!pip install -q groq\n",
        "!pip install -q langchain\n",
        "!pip install -q langchain-core\n",
        "!pip install -q langchain-community\n",
        "!pip install -q chromadb\n",
        "!pip install -q sentence-transformers\n",
        "!pip install -q requests\n",
        "!pip install -q beautifulsoup4\n",
        "!pip install -q streamlit\n",
        "!apt-get update -y\n",
        "!apt-get install -y ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, traceback, time, glob\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from yt_dlp import YoutubeDL\n",
        "from groq import Groq\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from IPython.display import clear_output, display\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "m6Mh_ROrAd8s"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import os, re, glob, shutil\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from yt_dlp import YoutubeDL\n",
        "from groq import Groq\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# ==========================================\n",
        "# 1. CONFIG & UI STYLING\n",
        "# ==========================================\n",
        "st.set_page_config(page_title=\"EduMentor AI\", page_icon=\"ğŸ“\", layout=\"wide\")\n",
        "\n",
        "# Custom CSS for a \"Decent & Beautiful\" look\n",
        "st.markdown(\"\"\"\n",
        "    <style>\n",
        "    /* Main Background and Text */\n",
        "    .stApp {\n",
        "        background-color: #f9f9f9;\n",
        "        color: #333333;\n",
        "    }\n",
        "    /* Sidebar Styling */\n",
        "    [data-testid=\"stSidebar\"] {\n",
        "        background-color: #ffffff;\n",
        "        border-right: 1px solid #e6e6e6;\n",
        "    }\n",
        "    /* Headers */\n",
        "    h1, h2, h3 {\n",
        "        font-family: 'Helvetica Neue', sans-serif;\n",
        "        font-weight: 600;\n",
        "        color: #1a1a1a;\n",
        "    }\n",
        "    /* Chat Bubbles */\n",
        "    .stChatMessage {\n",
        "        background-color: #ffffff;\n",
        "        border-radius: 15px;\n",
        "        border: 1px solid #eee;\n",
        "        box-shadow: 0 2px 5px rgba(0,0,0,0.05);\n",
        "    }\n",
        "    /* Button Styling */\n",
        "    div.stButton > button {\n",
        "        background-color: #2e86de;\n",
        "        color: white;\n",
        "        border-radius: 8px;\n",
        "        border: none;\n",
        "        padding: 0.5rem 1rem;\n",
        "        transition: all 0.3s ease;\n",
        "    }\n",
        "    div.stButton > button:hover {\n",
        "        background-color: #0984e3;\n",
        "        transform: scale(1.02);\n",
        "    }\n",
        "    </style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# ==========================================\n",
        "# 2. BACKEND LOGIC (Adapted from your script)\n",
        "# ==========================================\n",
        "\n",
        "PERSIST_DIR = \"./edumentor_memory\"\n",
        "\n",
        "def extract_video_id(url):\n",
        "    m = re.search(r\"(?:v=|youtu\\.be/)([A-Za-z0-9_-]+)\", url)\n",
        "    return m.group(1) if m else None\n",
        "\n",
        "def get_transcript_api(video_id):\n",
        "    try:\n",
        "        parts = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "        return \" \".join([p[\"text\"] for p in parts])\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def get_transcript_scraper(video_id):\n",
        "    try:\n",
        "        url = f\"https://ytscribe.com/v/{video_id}\"\n",
        "        resp = requests.get(url, timeout=12)\n",
        "        if resp.status_code == 200:\n",
        "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "            captions = soup.find_all(\"div\", {\"class\": \"caption-line\"})\n",
        "            if captions:\n",
        "                return \"\\n\".join([c.get_text(strip=True) for c in captions])\n",
        "    except:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "def download_audio(url, cookies_path=None):\n",
        "    ydl_opts = {\n",
        "        \"format\": \"bestaudio/best\",\n",
        "        \"outtmpl\": \"audio.%(ext)s\",\n",
        "        \"noplaylist\": True,\n",
        "        \"quiet\": True,\n",
        "        \"no_warnings\": True,\n",
        "        \"postprocessors\": [{\"key\": \"FFmpegExtractAudio\", \"preferredcodec\": \"mp3\", \"preferredquality\": \"128\"}],\n",
        "    }\n",
        "    if cookies_path:\n",
        "        ydl_opts[\"cookiefile\"] = cookies_path\n",
        "\n",
        "    # Cleanup old files\n",
        "    if os.path.exists(\"audio.mp3\"): os.remove(\"audio.mp3\")\n",
        "\n",
        "    with YoutubeDL(ydl_opts) as ydl:\n",
        "        ydl.download([url])\n",
        "    return \"audio.mp3\" if os.path.exists(\"audio.mp3\") else None\n",
        "\n",
        "def whisper_transcribe(client, audio_file):\n",
        "    # Simple wrapper for the logic in your script\n",
        "    def transcribe_file(path):\n",
        "        with open(path, \"rb\") as f:\n",
        "            return client.audio.transcriptions.create(file=(path, f.read()), model=\"whisper-large-v3\").text\n",
        "\n",
        "    try:\n",
        "        return transcribe_file(audio_file)\n",
        "    except Exception as e:\n",
        "        if \"413\" in str(e): # Too large\n",
        "            # Compression\n",
        "            os.system(f\"ffmpeg -i {audio_file} -ar 16000 -ac 1 -b:a 32k compressed.mp3 -y\")\n",
        "            try:\n",
        "                return transcribe_file(\"compressed.mp3\")\n",
        "            except:\n",
        "                # Chunking\n",
        "                os.system(f\"ffmpeg -i compressed.mp3 -f segment -segment_time 600 -c copy chunk_%03d.mp3 -y\")\n",
        "                full_text = \"\"\n",
        "                for c in sorted(glob.glob(\"chunk_*.mp3\")):\n",
        "                    full_text += transcribe_file(c) + \" \"\n",
        "                return full_text\n",
        "        raise e\n",
        "\n",
        "@st.cache_resource\n",
        "def setup_embedding_model():\n",
        "    return HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "def build_vector_store(text):\n",
        "    # Reset DB for new video\n",
        "    if os.path.exists(PERSIST_DIR):\n",
        "        shutil.rmtree(PERSIST_DIR)\n",
        "\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=120)\n",
        "    chunks = splitter.split_text(text)\n",
        "    embed = setup_embedding_model()\n",
        "    vectordb = Chroma.from_texts(texts=chunks, embedding=embed, persist_directory=PERSIST_DIR)\n",
        "    return vectordb\n",
        "\n",
        "def get_mentor_response(client, question, vectordb):\n",
        "    docs = vectordb.similarity_search(question, k=4)\n",
        "    context = \"\\n\\n\".join([d.page_content for d in docs])\n",
        "    prompt = f\"\"\"\n",
        "    You are an educational AI mentor. Use the context below to answer the student's question clearly and encouragingly.\n",
        "\n",
        "    CONTEXT:\n",
        "    {context}\n",
        "\n",
        "    QUESTION: {question}\n",
        "\n",
        "    ANSWER:\n",
        "    \"\"\"\n",
        "    resp = client.chat.completions.create(\n",
        "        model=\"llama-3.1-8b-instant\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=700,\n",
        "    )\n",
        "    return resp.choices[0].message.content\n",
        "\n",
        "# ==========================================\n",
        "# 3. FRONTEND LAYOUT\n",
        "# ==========================================\n",
        "\n",
        "# --- Sidebar ---\n",
        "with st.sidebar:\n",
        "    st.title(\"âš™ï¸ Settings\")\n",
        "    groq_key = st.text_input(\"Groq API Key\", type=\"password\", placeholder=\"gsk_...\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.subheader(\"ğŸª Optional Cookies\")\n",
        "    cookies_file = st.file_uploader(\"Upload cookies.txt (for age-restricted videos)\", type=[\"txt\"])\n",
        "    cookies_path = None\n",
        "    if cookies_file:\n",
        "        with open(\"cookies.txt\", \"wb\") as f:\n",
        "            f.write(cookies_file.getbuffer())\n",
        "        cookies_path = \"cookies.txt\"\n",
        "        st.success(\"Cookies loaded!\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    if st.button(\"Clear Chat History\"):\n",
        "        st.session_state.messages = []\n",
        "        st.rerun()\n",
        "\n",
        "# --- Main Area ---\n",
        "st.title(\"ğŸ“ EduMentor AI\")\n",
        "st.caption(\"Turn any YouTube video into an interactive learning session.\")\n",
        "\n",
        "# Initialize Session State\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "if \"vectordb\" not in st.session_state:\n",
        "    st.session_state.vectordb = None\n",
        "if \"current_video\" not in st.session_state:\n",
        "    st.session_state.current_video = None\n",
        "\n",
        "# Video Input Section\n",
        "col1, col2 = st.columns([3, 1])\n",
        "with col1:\n",
        "    video_url = st.text_input(\"YouTube Video URL\", placeholder=\"https://www.youtube.com/watch?v=...\")\n",
        "with col2:\n",
        "    process_btn = st.button(\"Analyze Video\", use_container_width=True)\n",
        "\n",
        "# Processing Logic\n",
        "if process_btn and video_url and groq_key:\n",
        "    st.session_state.current_video = video_url\n",
        "    client = Groq(api_key=groq_key)\n",
        "\n",
        "    with st.status(\"ğŸš€ Processing content...\", expanded=True) as status:\n",
        "        try:\n",
        "            # 1. Get Transcript\n",
        "            st.write(\"ğŸ“¥ Extracting transcript...\")\n",
        "            transcript = None\n",
        "            vid_id = extract_video_id(video_url)\n",
        "\n",
        "            # Method A: API\n",
        "            transcript = get_transcript_api(vid_id)\n",
        "\n",
        "            # Method B: Scraper\n",
        "            if not transcript:\n",
        "                st.write(\"âš ï¸ API failed, trying scraper...\")\n",
        "                transcript = get_transcript_scraper(vid_id)\n",
        "\n",
        "            # Method C: Whisper\n",
        "            if not transcript:\n",
        "                st.write(\"ğŸ”Š Using Whisper AI (Audio processing)...\")\n",
        "                audio_file = download_audio(video_url, cookies_path)\n",
        "                if audio_file:\n",
        "                    transcript = whisper_transcribe(client, audio_file)\n",
        "\n",
        "            if not transcript:\n",
        "                status.update(label=\"âŒ Failed to get transcript.\", state=\"error\")\n",
        "                st.error(\"Could not extract text from this video.\")\n",
        "                st.stop()\n",
        "\n",
        "            # 2. Build Memory\n",
        "            st.write(\"ğŸ§  Building Knowledge Base...\")\n",
        "            st.session_state.vectordb = build_vector_store(transcript)\n",
        "\n",
        "            status.update(label=\"âœ… Ready to teach!\", state=\"complete\")\n",
        "            st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"I've watched the video! What would you like to know?\"}]\n",
        "\n",
        "        except Exception as e:\n",
        "            status.update(label=\"âŒ Error occurred\", state=\"error\")\n",
        "            st.error(f\"Error: {str(e)}\")\n",
        "\n",
        "elif process_btn and not groq_key:\n",
        "    st.warning(\"Please enter your Groq API Key in the sidebar.\")\n",
        "\n",
        "# Chat Interface\n",
        "if st.session_state.vectordb:\n",
        "    # Display Chat History\n",
        "    for msg in st.session_state.messages:\n",
        "        with st.chat_message(msg[\"role\"]):\n",
        "            st.markdown(msg[\"content\"])\n",
        "\n",
        "    # Handle User Input\n",
        "    if prompt := st.chat_input(\"Ask a question about the video...\"):\n",
        "        # Add user message\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.markdown(prompt)\n",
        "\n",
        "        # Generate Answer\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            with st.spinner(\"Thinking...\"):\n",
        "                client = Groq(api_key=groq_key)\n",
        "                response = get_mentor_response(client, prompt, st.session_state.vectordb)\n",
        "                st.markdown(response)\n",
        "\n",
        "        # Add assistant message\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "else:\n",
        "    if not process_btn:\n",
        "        st.info(\"Paste a URL above and click 'Analyze Video' to start.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Zi_egxSAfo3",
        "outputId": "c9eb7074-b6a7-4722-e5a1-4e477d8d288c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Run Streamlit with Ngrok\n",
        "!pip install -q pyngrok\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "import time\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# Paste your token inside the quotes below\n",
        "NGROK_AUTH_TOKEN = \"35skwcVhytskiqAcqiaw1xOGsF7_6m29BRgH7H1JVjKGNJJEY\"\n",
        "# ---------------------\n",
        "\n",
        "if NGROK_AUTH_TOKEN == \"\" or NGROK_AUTH_TOKEN == \"YOUR_NGROK_TOKEN_HERE\":\n",
        "    print(\"âŒ Error: Please replace 'YOUR_NGROK_TOKEN_HERE' with your actual ngrok Authtoken.\")\n",
        "else:\n",
        "    # 1. Authenticate\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "    # 2. Kill any existing streamlit/ngrok processes to prevent conflicts\n",
        "    os.system(\"pkill streamlit\")\n",
        "    ngrok.kill()\n",
        "\n",
        "    # 3. Run Streamlit in the background using nohup\n",
        "    # We direct output to /dev/null to keep the cell clean, or to a log file for debugging\n",
        "    os.system(\"nohup streamlit run app.py --server.port 8501 > streamlit.log 2>&1 &\")\n",
        "\n",
        "    # 4. Give Streamlit a moment to start\n",
        "    time.sleep(3)\n",
        "\n",
        "    # 5. Open the Ngrok Tunnel\n",
        "    try:\n",
        "        public_url = ngrok.connect(8501).public_url\n",
        "        print(f\"ğŸš€ Streamlit is ready! Click here: {public_url}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error connecting ngrok: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJUDyMB6GyiY",
        "outputId": "52849d4b-d4c6-4fa2-b697-b5a25ddb0fab"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Streamlit is ready! Click here: https://wilfred-snitchier-inoffensively.ngrok-free.dev\n"
          ]
        }
      ]
    }
  ]
}